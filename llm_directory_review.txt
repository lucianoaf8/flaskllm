📁 Folder: llm

📄 Files in folder:
- __init__.py
- base_llm_handler.py
- factory.py
- openai_handler.py
- openai_handler_v2.py
- openai_direct.py
- anthropic_handler.py
- conversation_storage.py
- template_storage.py

🧾 Summary of Major Issues:
- Empty `__init__.py` file doesn't expose key components
- `base_llm_handler.py` contains unrelated documentation blueprint code
- Multiple OpenAI handler implementations with duplicated code
- Incorrect file references in file headers (wrong file names)
- Extensive code duplication across handler implementations
- Missing proper abstract base class for LLM handlers
- Inconsistent error handling between different handlers
- Interface defined as a Protocol but not used consistently

🔍 Detailed Findings:

--- file: __init__.py ---
Purpose:
> Should expose key components from the LLM module for easier importing

Issues Found:
- File is completely empty
- Doesn't expose any of the module's functionality
- Makes imports more verbose than necessary

Fixes Required:
- Add proper exports to simplify importing key components
- Add module docstring explaining the purpose of the LLM module

--- file: base_llm_handler.py ---
Purpose:
> Should define the base interface for all LLM handlers

Issues Found:
- Contains unrelated documentation blueprint code for Flask routes
- Missing actual LLM handler interface definition
- Content completely unrelated to filename

Fixes Required:
- Replace with proper abstract base class or interface for LLM handlers
- Define required methods that all handlers must implement
- Add proper documentation and type hints

--- file: factory.py ---
Purpose:
> Implements factory pattern for creating LLM handlers based on configured provider

Issues Found:
- Uses runtime_checkable Protocol for LLMHandler instead of abstract base class
- Has multiple fallback mechanisms without clear documentation
- Contains redundant validation (checks settings.openai_api_key multiple times)
- Inconsistent application of caching (only applied to the standard handler)
- Complex nested exception handling

Fixes Required:
- Simplify factory with clearer fallback logic
- Apply caching consistently across all handlers
- Use abstract base class instead of Protocol for better type checking
- Improve error messages and logging

--- file: openai_handler.py ---
Purpose:
> Implements the original LLM handler for OpenAI's API

Issues Found:
- Duplicated system prompt creation logic found in all handlers
- Inconsistent error handling compared to v2 handler
- Does not handle all error types from the OpenAI API
- Uses outdated OpenAI client library patterns

Fixes Required:
- Extract common system prompt logic to a shared utility
- Standardize error handling with newer handler versions
- Update to use the latest OpenAI client patterns

--- file: openai_handler_v2.py ---
Purpose:
> Implements a more robust LLM handler for OpenAI's API with better error handling

Issues Found:
- Duplicates most of the code from openai_handler.py
- Contains the same system prompt creation logic
- Inconsistent error message formatting compared to other handlers
- Uses 'extra' parameter for logging instead of direct keyword arguments

Fixes Required:
- Extract duplicated code to shared utilities
- Standardize error message formatting
- Standardize logging patterns

--- file: openai_direct.py ---
Purpose:
> Implements a direct HTTP handler for OpenAI without using their client library

Issues Found:
- Duplicates most of the code from other OpenAI handlers
- Contains the same system prompt creation logic
- Uses different HTTP error handling approach
- Hard-coded API URL might need to be configurable
- Incorrect hashbang (#!/usr/bin/env python3) for a module file

Fixes Required:
- Extract duplicated code to shared utilities
- Standardize error handling patterns
- Make API URL configurable
- Remove unnecessary hashbang

--- file: anthropic_handler.py ---
Purpose:
> Implements the LLM handler for Anthropic's Claude API

Issues Found:
- Duplicates system prompt creation logic found in OpenAI handlers
- Some inconsistencies in error handling compared to OpenAI handlers
- Missing proper type hints for ContentBlock from anthropic library
- Lacks comprehensive model version handling

Fixes Required:
- Extract common system prompt logic to a shared utility
- Standardize error handling across all handlers
- Improve type hints and validation
- Add support for different Claude model versions

--- file: conversation_storage.py ---
Purpose:
> Defines models and storage for conversation history

Issues Found:
- Incorrect file reference in header (# llm/conversation.py)
- No actual storage implementation, just model definitions
- Missing methods for persisting conversations
- No integration with cache system defined elsewhere

Fixes Required:
- Fix file reference in header
- Implement actual storage functionality
- Add persistence methods
- Integrate with existing cache system

--- file: template_storage.py ---
Purpose:
> Defines models and storage for prompt templates

Issues Found:
- Incorrect file reference in header (# llm/templates.py)
- No actual storage implementation, just model definitions
- Missing methods for template CRUD operations
- Template variable substitution lacks error handling for unknown variables

Fixes Required:
- Fix file reference in header
- Implement actual storage functionality
- Add CRUD operations for templates
- Improve variable substitution with better error handling
